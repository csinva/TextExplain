{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 19:01:06.056586: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-07 19:01:06.056633: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/accounts/projects/vision/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from spacy.lang.en import English\n",
    "import argparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import sklearn\n",
    "import warnings\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to aggregate embeddings run for a particular order of n-gram here (we'll just add them up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in os.listdir(config.data_dir):\n",
    "    processed_dir = oj(config.data_dir, dset)\n",
    "    dir_names = [f for f in sorted(os.listdir(processed_dir))\n",
    "                 if not '-all' in f\n",
    "                ]\n",
    "\n",
    "    for i in tqdm(range(len(dir_names))):\n",
    "        s = dir_names[i]\n",
    "        start = s.index('=') + 1\n",
    "        end = s.index('_')\n",
    "        num = int(s[start: end])\n",
    "#         if num > 1:\n",
    "        print('Trying', s)\n",
    "        s_new = s + '-all'\n",
    "        \"\"\"\n",
    "        if os.path.exists(oj(processed_dir, s_new)):\n",
    "            print('\\tdone already!')\n",
    "            continue\n",
    "        \"\"\"\n",
    "\n",
    "        pre = s[:start]\n",
    "        end = s[end:]\n",
    "        all_exist = True\n",
    "        num_missing = []\n",
    "        for num_small in range(1, num):\n",
    "            fname_small = pre + str(num_small) + end\n",
    "            all_exist = all_exist and (fname_small in dir_names)\n",
    "            if not fname_small in dir_names:\n",
    "                num_missing.append(num_small)\n",
    "\n",
    "        if not all_exist:\n",
    "            print('\\tmissing small_ngrams: ', num_missing)\n",
    "            continue\n",
    "\n",
    "        # load dset\n",
    "        reloaded_dataset = load_from_disk(oj(processed_dir, s))\n",
    "        X_train = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "        X_val = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "\n",
    "        for num_small in range(1, num + 1):\n",
    "            fname_small = pre + str(num_small) + end            \n",
    "            reloaded_dataset = load_from_disk(oj(processed_dir, fname_small))\n",
    "            X_train_small = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "            X_val_small = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "\n",
    "            X_train += X_train_small\n",
    "            X_val += X_val_small\n",
    "\n",
    "        os.makedirs(oj(processed_dir, s_new), exist_ok=True)\n",
    "        mu = X_train.mean(axis=0)\n",
    "        sigma = X_train.std(axis=0)\n",
    "        r = {\n",
    "            'X_train': X_train,\n",
    "            'X_val': X_val,\n",
    "            'mean': mu,\n",
    "            'sigma': sigma,\n",
    "        }\n",
    "        pkl.dump(r, open(oj(processed_dir, s_new, 'data.pkl'), 'wb'))\n",
    "        print('\\tsuccess!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
