{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a simple demo of how to use `EmbGAMClassifier`. It follows a simple sklearn-style interface, but leverage language models to extract embeddings, so may be slow to run during training. At test time, it converts to a simple linear model, making it extremely fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imodelsx import EmbGAMClassifier\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load some data\n",
    "Here, we load some training/validation data from the rotten-tomatoes movie dataset. To make things fast, we restrict our training and testing datasets to only 300 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/.embgam/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using custom data configuration default\n",
      "Found cached dataset rotten_tomatoes (/home/chansingh/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0982d64d02e4fd590c61b5f0e28d996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset rotten_tomatoes (/home/chansingh/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f0acf568474d40b7dded0b6d86eda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = datasets.load_dataset('rotten_tomatoes')['train']\n",
    "dset = dset.select(np.random.choice(len(dset), size=300, replace=False))\n",
    "\n",
    "dset_val = datasets.load_dataset('rotten_tomatoes')['validation']\n",
    "dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit EmbGAMClassifier\n",
    "Fitting EmbGAM is a simple function call! EmbGAM takes a few hyperparameters, which you can explore [here](https://csinva.io/emb-gam/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.5 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "m = EmbGAMClassifier(\n",
    "    checkpoint='textattack/distilbert-base-uncased-rotten-tomatoes',\n",
    "    ngrams=2,\n",
    "    all_ngrams=True, # also use lower-order ngrams\n",
    ")\n",
    "m.fit(dset['text'], dset['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "We now have a linear model of ngrams. The `fit` function above has precomputed the linear coefficients for ngrams it saw during training and saved them to `m.coefs_dict_` Let's take a look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ngram coefficients:  7437\n",
      "Most positive ngrams\n",
      "\t and captivating 0.38\n",
      "\t and poignant 0.37\n",
      "\t a wonderfully 0.37\n",
      "\t a captivatingly 0.37\n",
      "\t and exquisitely 0.36\n",
      "\t a poignant 0.36\n",
      "\t captivatingly 0.35\n",
      "\t a tremendous 0.35\n",
      "Most negative ngrams\n",
      "\t cliched . -0.32\n",
      "\t cliched and -0.32\n",
      "\t mediocre , -0.31\n",
      "\t too formulaic -0.31\n",
      "\t tedious . -0.31\n",
      "\t and overproduced -0.3\n",
      "\t and cliched -0.3\n",
      "\t cliches . -0.29\n"
     ]
    }
   ],
   "source": [
    "print('Total ngram coefficients: ', len(m.coefs_dict_))\n",
    "print('Most positive ngrams')\n",
    "for k, v in sorted(m.coefs_dict_.items(), key=lambda item: item[1], reverse=True)[:8]:\n",
    "    print('\\t', k, round(v, 2))\n",
    "print('Most negative ngrams')\n",
    "for k, v in sorted(m.coefs_dict_.items(), key=lambda item: item[1])[:8]:\n",
    "    print('\\t', k, round(v, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "Now, let's take a look at how we make predictions. This is very fast, as it just uses the precomputed dictionary `m.coefs_dict_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 12569.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_train 0.7866666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 12643.60it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = m.predict(dset['text'])\n",
    "print('acc_train', np.mean(preds == dset['label']))\n",
    "preds_proba = m.predict_proba(dset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 5479.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_val 0.6633333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/chansingh/emb-gam/embgam/embgam.py:271: UserWarning: Saw an unseen ungram 6647 times. For better performance, call cache_linear_coefs on the test dataset before calling predict.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preds = m.predict(dset_val['text'])\n",
    "print('acc_val', np.mean(preds == dset_val['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we may want to infer the coefficients for ngrams we didn't see during training. To do this, we call the `cache_linear_coefs` function on the inputs for the test set. This adds the values for the unseen coefficients to the dictionary `m.coefs_dict_`. Then we can call `predict` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/distilbert-base-uncased-rotten-tomatoes were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 6359/6359 [00:25<00:00, 249.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefs_dict_ len 13748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 11890.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_val 0.7933333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m.cache_linear_coefs(dset_val['text'])\n",
    "preds = m.predict(dset_val['text'])\n",
    "print('acc_val', np.mean(preds == dset_val['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
