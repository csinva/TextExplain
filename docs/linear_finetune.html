<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imodelsx.linear_finetune API documentation</title>
<meta name="description" content="Simple scikit-learn interface for finetuning a single linear layer on top of LLM embeddings." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodelsx.linear_finetune</code></h1>
</header>
<section id="section-intro">
<p>Simple scikit-learn interface for finetuning a single linear layer on top of LLM embeddings.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Simple scikit-learn interface for finetuning a single linear layer on top of LLM embeddings.
&#34;&#34;&#34;
from numpy.typing import ArrayLike
import numpy as np
from scipy.special import softmax
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin
from sklearn.linear_model import LogisticRegressionCV, RidgeCV
from sklearn.utils.multiclass import unique_labels
from sklearn.utils.validation import check_is_fitted
from spacy.lang.en import English
from scipy.sparse import issparse
from sklearn.preprocessing import StandardScaler
import transformers
from tqdm import tqdm
import os
import os.path
import warnings
import pickle as pkl
import torch
from sklearn.exceptions import ConvergenceWarning

device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;


class LinearFinetune(BaseEstimator):
    def __init__(
        self,
        checkpoint: str = &#34;bert-base-uncased&#34;,
        layer: str = &#34;last_hidden_state&#34;,
        random_state=None,
        normalize_embs=False,
        cache_embs_dir: str = None,
        verbose: int = 0,
    ):
        &#34;&#34;&#34;LinearFinetune Class - use either LinearFinetuneClassifier or LinearFinetuneRegressor rather than initializing this class directly.

        Parameters
        ----------
        checkpoint: str
            Name of model checkpoint (i.e. to be fetch by huggingface)
        layer: str
            Name of layer to extract embeddings from
        random_state
            random seed for fitting
        normalize_embs
            whether to normalize embeddings before fitting linear model
        cache_embs_dir, optional
            if not None, directory to save embeddings into

        Example
        -------
        ```
        from imodelsx import LinearFinetuneClassifier
        import datasets
        import numpy as np

        # load data
        dset = datasets.load_dataset(&#39;rotten_tomatoes&#39;)[&#39;train&#39;]
        dset = dset.select(np.random.choice(len(dset), size=300, replace=False))
        dset_val = datasets.load_dataset(&#39;rotten_tomatoes&#39;)[&#39;validation&#39;]
        dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))


        # fit a simple one-layer finetune
        m = LinearFinetuneClassifier(
            checkpoint=&#39;distilbert-base-uncased&#39;,
        )
        m.fit(dset[&#39;text&#39;], dset[&#39;label&#39;])
        preds = m.predict(dset_val[&#39;text&#39;])
        acc = (preds == dset_val[&#39;label&#39;]).mean()
        print(&#39;validation acc&#39;, acc)
        ```
        &#34;&#34;&#34;
        self.checkpoint = checkpoint
        self.layer = layer
        self.random_state = random_state
        self.normalize_embs = normalize_embs
        self.cache_embs_dir = cache_embs_dir
        self.verbose = verbose
        self._initialize_checkpoint_and_tokenizer()

    def _initialize_checkpoint_and_tokenizer(self):
        self.model = transformers.AutoModel.from_pretrained(self.checkpoint).to(device)
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.checkpoint)

    def fit(
        self,
        X_text: ArrayLike,
        y: ArrayLike,
    ):
        &#34;&#34;&#34;Extract embeddings then fit linear model

        Parameters
        ----------
        X_text: ArrayLike[str]
        y: ArrayLike[str]
        &#34;&#34;&#34;

        # metadata
        if isinstance(self, ClassifierMixin):
            self.classes_ = unique_labels(y)
        if self.random_state is not None:
            np.random.seed(self.random_state)

        # set up model
        if self.verbose:
            print(&#34;initializing model...&#34;)

        # get embs
        if self.verbose:
            print(&#34;calculating embeddings...&#34;)
        if self.cache_embs_dir is not None and os.path.exists(
            os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;)
        ):
            embs = pkl.load(open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;rb&#34;))
        else:
            embs = self._get_embs(X_text)
            if self.cache_embs_dir is not None:
                os.makedirs(self.cache_embs_dir, exist_ok=True)
                pkl.dump(
                    embs, open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;wb&#34;)
                )
        if self.normalize_embs:
            self.normalizer = StandardScaler()
            embs = self.normalizer.fit_transform(embs)

        # train linear
        warnings.filterwarnings(&#34;ignore&#34;, category=ConvergenceWarning)
        if self.verbose:
            print(&#34;training linear model...&#34;)
        if isinstance(self, ClassifierMixin):
            self.linear = LogisticRegressionCV()
        elif isinstance(self, RegressorMixin):
            self.linear = RidgeCV()
        self.linear.fit(embs, y)

        return self

    def _get_embs(self, X_text: ArrayLike):
        embs = []
        if isinstance(X_text, list):
            n = len(X_text)
        else:
            n = X_text.shape[0]
        for i in tqdm(range(n)):
            inputs = self.tokenizer(
                [X_text[i]], padding=&#34;max_length&#34;, truncation=True, return_tensors=&#34;pt&#34;
            )
            inputs = inputs.to(self.model.device)
            output = self.model(**inputs)
            emb = output[self.layer].cpu().detach().numpy()
            if len(emb.shape) == 3:  # includes seq_len
                emb = emb.mean(axis=1)
            embs.append(emb)
        return np.array(embs).squeeze()  # num_examples x embedding_size

    def predict(self, X_text):
        &#34;&#34;&#34;For regression returns continuous output.
        For classification, returns discrete output.
        &#34;&#34;&#34;
        check_is_fitted(self)
        embs = self._get_embs(X_text)
        if self.normalize_embs:
            embs = self.normalizer.transform(embs)
        return self.linear.predict(embs)

    def predict_proba(self, X_text):
        check_is_fitted(self)
        embs = self._get_embs(X_text)
        if self.normalize_embs:
            embs = self.normalizer.transform(embs)
        return self.linear.predict_proba(embs)


class LinearFinetuneRegressor(LinearFinetune, RegressorMixin):
    ...


class LinearFinetuneClassifier(LinearFinetune, ClassifierMixin):
    ...


if __name__ == &#34;__main__&#34;:
    import imodelsx.data

    dset, k = imodelsx.data.load_huggingface_dataset(
        &#34;rotten_tomatoes&#34;, binary_classification=False, subsample_frac=0.1
    )
    print(dset)
    print(dset[&#34;train&#34;])
    print(np.unique(dset[&#34;train&#34;][&#34;label&#34;]))

    clf = LinearFinetuneClassifier()
    clf.fit(dset[&#34;train&#34;][&#34;text&#34;], dset[&#34;train&#34;][&#34;label&#34;])

    print(&#34;predicting&#34;)
    preds = clf.predict(dset[&#34;test&#34;][&#34;text&#34;])
    print(preds.shape)

    print(&#34;predicting proba&#34;)
    preds_proba = clf.predict_proba(dset[&#34;test&#34;][&#34;text&#34;])
    print(preds_proba.shape)

    assert preds_proba.shape[0] == preds.shape[0]
    print(
        &#34;acc_train&#34;,
        np.mean(clf.predict(dset[&#34;train&#34;][&#34;text&#34;]) == dset[&#34;train&#34;][&#34;label&#34;]),
    )
    print(&#34;acc_test&#34;, np.mean(preds == dset[&#34;test&#34;][&#34;label&#34;]))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodelsx.linear_finetune.LinearFinetune"><code class="flex name class">
<span>class <span class="ident">LinearFinetune</span></span>
<span>(</span><span>checkpoint: str = 'bert-base-uncased', layer: str = 'last_hidden_state', random_state=None, normalize_embs=False, cache_embs_dir: str = None, verbose: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<p>LinearFinetune Class - use either LinearFinetuneClassifier or LinearFinetuneRegressor rather than initializing this class directly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of model checkpoint (i.e. to be fetch by huggingface)</dd>
<dt><strong><code>layer</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of layer to extract embeddings from</dd>
<dt><strong><code>random_state</code></strong></dt>
<dd>random seed for fitting</dd>
<dt><strong><code>normalize_embs</code></strong></dt>
<dd>whether to normalize embeddings before fitting linear model</dd>
<dt><strong><code>cache_embs_dir</code></strong>, <strong><code>optional</code></strong></dt>
<dd>if not None, directory to save embeddings into</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>from imodelsx import LinearFinetuneClassifier
import datasets
import numpy as np

# load data
dset = datasets.load_dataset('rotten_tomatoes')['train']
dset = dset.select(np.random.choice(len(dset), size=300, replace=False))
dset_val = datasets.load_dataset('rotten_tomatoes')['validation']
dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))


# fit a simple one-layer finetune
m = LinearFinetuneClassifier(
    checkpoint='distilbert-base-uncased',
)
m.fit(dset['text'], dset['label'])
preds = m.predict(dset_val['text'])
acc = (preds == dset_val['label']).mean()
print('validation acc', acc)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFinetune(BaseEstimator):
    def __init__(
        self,
        checkpoint: str = &#34;bert-base-uncased&#34;,
        layer: str = &#34;last_hidden_state&#34;,
        random_state=None,
        normalize_embs=False,
        cache_embs_dir: str = None,
        verbose: int = 0,
    ):
        &#34;&#34;&#34;LinearFinetune Class - use either LinearFinetuneClassifier or LinearFinetuneRegressor rather than initializing this class directly.

        Parameters
        ----------
        checkpoint: str
            Name of model checkpoint (i.e. to be fetch by huggingface)
        layer: str
            Name of layer to extract embeddings from
        random_state
            random seed for fitting
        normalize_embs
            whether to normalize embeddings before fitting linear model
        cache_embs_dir, optional
            if not None, directory to save embeddings into

        Example
        -------
        ```
        from imodelsx import LinearFinetuneClassifier
        import datasets
        import numpy as np

        # load data
        dset = datasets.load_dataset(&#39;rotten_tomatoes&#39;)[&#39;train&#39;]
        dset = dset.select(np.random.choice(len(dset), size=300, replace=False))
        dset_val = datasets.load_dataset(&#39;rotten_tomatoes&#39;)[&#39;validation&#39;]
        dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))


        # fit a simple one-layer finetune
        m = LinearFinetuneClassifier(
            checkpoint=&#39;distilbert-base-uncased&#39;,
        )
        m.fit(dset[&#39;text&#39;], dset[&#39;label&#39;])
        preds = m.predict(dset_val[&#39;text&#39;])
        acc = (preds == dset_val[&#39;label&#39;]).mean()
        print(&#39;validation acc&#39;, acc)
        ```
        &#34;&#34;&#34;
        self.checkpoint = checkpoint
        self.layer = layer
        self.random_state = random_state
        self.normalize_embs = normalize_embs
        self.cache_embs_dir = cache_embs_dir
        self.verbose = verbose
        self._initialize_checkpoint_and_tokenizer()

    def _initialize_checkpoint_and_tokenizer(self):
        self.model = transformers.AutoModel.from_pretrained(self.checkpoint).to(device)
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.checkpoint)

    def fit(
        self,
        X_text: ArrayLike,
        y: ArrayLike,
    ):
        &#34;&#34;&#34;Extract embeddings then fit linear model

        Parameters
        ----------
        X_text: ArrayLike[str]
        y: ArrayLike[str]
        &#34;&#34;&#34;

        # metadata
        if isinstance(self, ClassifierMixin):
            self.classes_ = unique_labels(y)
        if self.random_state is not None:
            np.random.seed(self.random_state)

        # set up model
        if self.verbose:
            print(&#34;initializing model...&#34;)

        # get embs
        if self.verbose:
            print(&#34;calculating embeddings...&#34;)
        if self.cache_embs_dir is not None and os.path.exists(
            os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;)
        ):
            embs = pkl.load(open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;rb&#34;))
        else:
            embs = self._get_embs(X_text)
            if self.cache_embs_dir is not None:
                os.makedirs(self.cache_embs_dir, exist_ok=True)
                pkl.dump(
                    embs, open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;wb&#34;)
                )
        if self.normalize_embs:
            self.normalizer = StandardScaler()
            embs = self.normalizer.fit_transform(embs)

        # train linear
        warnings.filterwarnings(&#34;ignore&#34;, category=ConvergenceWarning)
        if self.verbose:
            print(&#34;training linear model...&#34;)
        if isinstance(self, ClassifierMixin):
            self.linear = LogisticRegressionCV()
        elif isinstance(self, RegressorMixin):
            self.linear = RidgeCV()
        self.linear.fit(embs, y)

        return self

    def _get_embs(self, X_text: ArrayLike):
        embs = []
        if isinstance(X_text, list):
            n = len(X_text)
        else:
            n = X_text.shape[0]
        for i in tqdm(range(n)):
            inputs = self.tokenizer(
                [X_text[i]], padding=&#34;max_length&#34;, truncation=True, return_tensors=&#34;pt&#34;
            )
            inputs = inputs.to(self.model.device)
            output = self.model(**inputs)
            emb = output[self.layer].cpu().detach().numpy()
            if len(emb.shape) == 3:  # includes seq_len
                emb = emb.mean(axis=1)
            embs.append(emb)
        return np.array(embs).squeeze()  # num_examples x embedding_size

    def predict(self, X_text):
        &#34;&#34;&#34;For regression returns continuous output.
        For classification, returns discrete output.
        &#34;&#34;&#34;
        check_is_fitted(self)
        embs = self._get_embs(X_text)
        if self.normalize_embs:
            embs = self.normalizer.transform(embs)
        return self.linear.predict(embs)

    def predict_proba(self, X_text):
        check_is_fitted(self)
        embs = self._get_embs(X_text)
        if self.normalize_embs:
            embs = self.normalizer.transform(embs)
        return self.linear.predict_proba(embs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodelsx.linear_finetune.LinearFinetuneClassifier" href="#imodelsx.linear_finetune.LinearFinetuneClassifier">LinearFinetuneClassifier</a></li>
<li><a title="imodelsx.linear_finetune.LinearFinetuneRegressor" href="#imodelsx.linear_finetune.LinearFinetuneRegressor">LinearFinetuneRegressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.linear_finetune.LinearFinetune.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X_text: Union[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]], numpy._typing._nested_sequence._NestedSequence[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy._typing._nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]], y: Union[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]], numpy._typing._nested_sequence._NestedSequence[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy._typing._nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]])</span>
</code></dt>
<dd>
<div class="desc"><p>Extract embeddings then fit linear model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_text</code></strong> :&ensp;<code>ArrayLike[str]</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ArrayLike[str]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(
    self,
    X_text: ArrayLike,
    y: ArrayLike,
):
    &#34;&#34;&#34;Extract embeddings then fit linear model

    Parameters
    ----------
    X_text: ArrayLike[str]
    y: ArrayLike[str]
    &#34;&#34;&#34;

    # metadata
    if isinstance(self, ClassifierMixin):
        self.classes_ = unique_labels(y)
    if self.random_state is not None:
        np.random.seed(self.random_state)

    # set up model
    if self.verbose:
        print(&#34;initializing model...&#34;)

    # get embs
    if self.verbose:
        print(&#34;calculating embeddings...&#34;)
    if self.cache_embs_dir is not None and os.path.exists(
        os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;)
    ):
        embs = pkl.load(open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;rb&#34;))
    else:
        embs = self._get_embs(X_text)
        if self.cache_embs_dir is not None:
            os.makedirs(self.cache_embs_dir, exist_ok=True)
            pkl.dump(
                embs, open(os.path.join(self.cache_embs_dir, &#34;embs.pkl&#34;), &#34;wb&#34;)
            )
    if self.normalize_embs:
        self.normalizer = StandardScaler()
        embs = self.normalizer.fit_transform(embs)

    # train linear
    warnings.filterwarnings(&#34;ignore&#34;, category=ConvergenceWarning)
    if self.verbose:
        print(&#34;training linear model...&#34;)
    if isinstance(self, ClassifierMixin):
        self.linear = LogisticRegressionCV()
    elif isinstance(self, RegressorMixin):
        self.linear = RidgeCV()
    self.linear.fit(embs, y)

    return self</code></pre>
</details>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetune.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X_text)</span>
</code></dt>
<dd>
<div class="desc"><p>For regression returns continuous output.
For classification, returns discrete output.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X_text):
    &#34;&#34;&#34;For regression returns continuous output.
    For classification, returns discrete output.
    &#34;&#34;&#34;
    check_is_fitted(self)
    embs = self._get_embs(X_text)
    if self.normalize_embs:
        embs = self.normalizer.transform(embs)
    return self.linear.predict(embs)</code></pre>
</details>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetune.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X_text)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X_text):
    check_is_fitted(self)
    embs = self._get_embs(X_text)
    if self.normalize_embs:
        embs = self.normalizer.transform(embs)
    return self.linear.predict_proba(embs)</code></pre>
</details>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetune.set_fit_request"><code class="name flex">
<span>def <span class="ident">set_fit_request</span></span>(<span>self: <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a>, *, X_text: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>fit</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_text</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>X_text</code> parameter in <code>fit</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request"><code class="name flex">
<span>def <span class="ident">set_predict_proba_request</span></span>(<span>self: <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a>, *, X_text: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>predict_proba</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>predict_proba</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict_proba</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_text</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>X_text</code> parameter in <code>predict_proba</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetune.set_predict_request"><code class="name flex">
<span>def <span class="ident">set_predict_request</span></span>(<span>self: <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a>, *, X_text: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>predict</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_text</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>X_text</code> parameter in <code>predict</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetuneClassifier"><code class="flex name class">
<span>class <span class="ident">LinearFinetuneClassifier</span></span>
<span>(</span><span>checkpoint: str = 'bert-base-uncased', layer: str = 'last_hidden_state', random_state=None, normalize_embs=False, cache_embs_dir: str = None, verbose: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<p>LinearFinetune Class - use either LinearFinetuneClassifier or LinearFinetuneRegressor rather than initializing this class directly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of model checkpoint (i.e. to be fetch by huggingface)</dd>
<dt><strong><code>layer</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of layer to extract embeddings from</dd>
<dt><strong><code>random_state</code></strong></dt>
<dd>random seed for fitting</dd>
<dt><strong><code>normalize_embs</code></strong></dt>
<dd>whether to normalize embeddings before fitting linear model</dd>
<dt><strong><code>cache_embs_dir</code></strong>, <strong><code>optional</code></strong></dt>
<dd>if not None, directory to save embeddings into</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>from imodelsx import LinearFinetuneClassifier
import datasets
import numpy as np

# load data
dset = datasets.load_dataset('rotten_tomatoes')['train']
dset = dset.select(np.random.choice(len(dset), size=300, replace=False))
dset_val = datasets.load_dataset('rotten_tomatoes')['validation']
dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))


# fit a simple one-layer finetune
m = LinearFinetuneClassifier(
    checkpoint='distilbert-base-uncased',
)
m.fit(dset['text'], dset['label'])
preds = m.predict(dset_val['text'])
acc = (preds == dset_val['label']).mean()
print('validation acc', acc)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFinetuneClassifier(LinearFinetune, ClassifierMixin):
    ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
<li>sklearn.base.ClassifierMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.linear_finetune.LinearFinetuneClassifier.set_score_request"><code class="name flex">
<span>def <span class="ident">set_score_request</span></span>(<span>self: <a title="imodelsx.linear_finetune.LinearFinetuneClassifier" href="#imodelsx.linear_finetune.LinearFinetuneClassifier">LinearFinetuneClassifier</a>, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodelsx.linear_finetune.LinearFinetuneClassifier" href="#imodelsx.linear_finetune.LinearFinetuneClassifier">LinearFinetuneClassifier</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>score</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>score</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>score</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>score</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></b></code>:
<ul class="hlist">
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.fit" href="#imodelsx.linear_finetune.LinearFinetune.fit">fit</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.predict" href="#imodelsx.linear_finetune.LinearFinetune.predict">predict</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_fit_request" href="#imodelsx.linear_finetune.LinearFinetune.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request">set_predict_proba_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_request">set_predict_request</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="imodelsx.linear_finetune.LinearFinetuneRegressor"><code class="flex name class">
<span>class <span class="ident">LinearFinetuneRegressor</span></span>
<span>(</span><span>checkpoint: str = 'bert-base-uncased', layer: str = 'last_hidden_state', random_state=None, normalize_embs=False, cache_embs_dir: str = None, verbose: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<p>LinearFinetune Class - use either LinearFinetuneClassifier or LinearFinetuneRegressor rather than initializing this class directly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of model checkpoint (i.e. to be fetch by huggingface)</dd>
<dt><strong><code>layer</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of layer to extract embeddings from</dd>
<dt><strong><code>random_state</code></strong></dt>
<dd>random seed for fitting</dd>
<dt><strong><code>normalize_embs</code></strong></dt>
<dd>whether to normalize embeddings before fitting linear model</dd>
<dt><strong><code>cache_embs_dir</code></strong>, <strong><code>optional</code></strong></dt>
<dd>if not None, directory to save embeddings into</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>from imodelsx import LinearFinetuneClassifier
import datasets
import numpy as np

# load data
dset = datasets.load_dataset('rotten_tomatoes')['train']
dset = dset.select(np.random.choice(len(dset), size=300, replace=False))
dset_val = datasets.load_dataset('rotten_tomatoes')['validation']
dset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))


# fit a simple one-layer finetune
m = LinearFinetuneClassifier(
    checkpoint='distilbert-base-uncased',
)
m.fit(dset['text'], dset['label'])
preds = m.predict(dset_val['text'])
acc = (preds == dset_val['label']).mean()
print('validation acc', acc)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFinetuneRegressor(LinearFinetune, RegressorMixin):
    ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.linear_finetune.LinearFinetuneRegressor.set_score_request"><code class="name flex">
<span>def <span class="ident">set_score_request</span></span>(<span>self: <a title="imodelsx.linear_finetune.LinearFinetuneRegressor" href="#imodelsx.linear_finetune.LinearFinetuneRegressor">LinearFinetuneRegressor</a>, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodelsx.linear_finetune.LinearFinetuneRegressor" href="#imodelsx.linear_finetune.LinearFinetuneRegressor">LinearFinetuneRegressor</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>score</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>score</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>score</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>score</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></b></code>:
<ul class="hlist">
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.fit" href="#imodelsx.linear_finetune.LinearFinetune.fit">fit</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.predict" href="#imodelsx.linear_finetune.LinearFinetune.predict">predict</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_fit_request" href="#imodelsx.linear_finetune.LinearFinetune.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request">set_predict_proba_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_request">set_predict_request</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodelsx" href="index.html">imodelsx</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodelsx.linear_finetune.LinearFinetune" href="#imodelsx.linear_finetune.LinearFinetune">LinearFinetune</a></code></h4>
<ul class="">
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.fit" href="#imodelsx.linear_finetune.LinearFinetune.fit">fit</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.predict" href="#imodelsx.linear_finetune.LinearFinetune.predict">predict</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.predict_proba" href="#imodelsx.linear_finetune.LinearFinetune.predict_proba">predict_proba</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_fit_request" href="#imodelsx.linear_finetune.LinearFinetune.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_proba_request">set_predict_proba_request</a></code></li>
<li><code><a title="imodelsx.linear_finetune.LinearFinetune.set_predict_request" href="#imodelsx.linear_finetune.LinearFinetune.set_predict_request">set_predict_request</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodelsx.linear_finetune.LinearFinetuneClassifier" href="#imodelsx.linear_finetune.LinearFinetuneClassifier">LinearFinetuneClassifier</a></code></h4>
<ul class="">
<li><code><a title="imodelsx.linear_finetune.LinearFinetuneClassifier.set_score_request" href="#imodelsx.linear_finetune.LinearFinetuneClassifier.set_score_request">set_score_request</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodelsx.linear_finetune.LinearFinetuneRegressor" href="#imodelsx.linear_finetune.LinearFinetuneRegressor">LinearFinetuneRegressor</a></code></h4>
<ul class="">
<li><code><a title="imodelsx.linear_finetune.LinearFinetuneRegressor.set_score_request" href="#imodelsx.linear_finetune.LinearFinetuneRegressor.set_score_request">set_score_request</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>