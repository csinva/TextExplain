<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imodelsx.auglinear.embed API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodelsx.auglinear.embed</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from os.path import join as oj
from datasets import Dataset
from tqdm import tqdm
import torch
import numpy as np
from torch.utils.data import DataLoader
import imodelsx.util
from typing import List


def embed_and_sum_function(
    example,
    model,
    ngrams: int,
    tokenizer_embeddings,
    tokenizer_ngrams,
    checkpoint: str,
    dataset_key_text: str = None,
    layer: str = &#34;last_hidden_state&#34;,
    padding: str = True,
    batch_size: int = 8,
    parsing: str = &#34;&#34;,
    nlp_chunks=None,
    all_ngrams: bool = False,
    fit_with_ngram_decomposition: bool = True,
    embedding_prefix: str = &#34;Represent the short phrase for sentiment classification: &#34;,
    embedding_suffix: str = &#34;&#34;,
    embedding_strategy: str = &#39;mean&#39;,
    sum_embeddings=True,
    prune_stopwords=False,
):
    &#34;&#34;&#34;Get summed embeddings for a single example

    Params
    ------
    ngrams: int
        What order of ngrams to use (1 for unigrams, 2 for bigrams, ...)
    dataset_key_text:
        str that identifies where data examples are stored, e.g. &#34;sentence&#34; for sst2
    tokenizer_embeddings
        tokenizing for the embedding model
    tokenizer_ngrams
        tokenizing the ngrams (word-based tokenization is more interpretable)
    layer: str
        which layer to extract embeddings from
    batch_size: int
        batch size for simultaneously running ngrams (for a single example)
    parsing: str
        whether to use parsing rather than extracting all ngrams
    nlp_chunks
        if parsing is not empty string, a parser that extracts specific ngrams
    fit_with_ngram_decomposition
        whether to fit the model with ngram decomposition (if not just use the standard sentence)
    embedding_prefix
        if checkpoint is an instructor/autoregressive model, prepend this prompt
    embedding_suffix
        if checkpoint is an autoregressive model, append this prompt
    embedding_strategy: str
        &#39;mean&#39;: compute mean over ngram tokens
        &#39;next_token_distr&#39;: use next token distribution as an embedding (requires AutoModelForCausalLM checkpoint)
    all_ngrams: bool
        whether to include all ngrams of lower order
    &#34;&#34;&#34;

    # convert to list of strings
    seqs = _get_seqs(
        example, dataset_key_text, fit_with_ngram_decomposition,
        ngrams, tokenizer_ngrams, parsing, nlp_chunks, all_ngrams, prune_stopwords)
    if embedding_strategy == &#39;next_token_distr&#39;:
        seqs = [f&#39;{embedding_prefix}{x_i}{embedding_suffix}&#39; for x_i in seqs]

    if not checkpoint.startswith(&#34;hkunlp/instructor&#34;) and (
        not hasattr(tokenizer_embeddings, &#34;pad_token&#34;)
        or tokenizer_embeddings.pad_token is None
    ):
        tokenizer_embeddings.pad_token = tokenizer_embeddings.eos_token

    # compute embeddings
    embs = []
    if checkpoint.startswith(&#34;hkunlp/instructor&#34;):
        embs = model.encode(
            [[embedding_prefix, x_i] for x_i in seqs], batch_size=batch_size
        )
    else:
        tokens = tokenizer_embeddings(
            seqs, padding=padding, truncation=True, return_tensors=&#34;pt&#34;
        )

        ds = Dataset.from_dict(tokens).with_format(&#34;torch&#34;)

        for batch in DataLoader(ds, batch_size=batch_size, shuffle=False):
            batch = {k: v.to(model.device) for k, v in batch.items()}

            with torch.no_grad():
                output = model(**batch)
            torch.cuda.empty_cache()
            if embedding_strategy == &#34;next_token_distr&#34;:
                emb = _next_token_distr_with_mask(
                    output[&#34;logits&#34;], batch[&#34;attention_mask&#34;]
                )
            else:
                if layer == &#34;pooler_output&#34;:
                    emb = output[&#34;pooler_output&#34;]
                elif layer == &#34;last_hidden_state_mean&#34; or layer == &#34;last_hidden_state&#34;:
                    # extract (batch_size, seq_len, hidden_size)
                    emb = output[&#34;last_hidden_state&#34;]

                    # convert to (batch_size, hidden_size)
                    emb = _mean_with_mask(emb, batch[&#34;attention_mask&#34;])

                elif &#34;hidden_states&#34; in output.keys():
                    # extract (layer x (batch_size, seq_len, hidden_size))
                    h = output[&#34;hidden_states&#34;]

                    # convert to (batch_size, seq_len, hidden_size)
                    emb = h[0]

                    # convert to (batch_size, hidden_size)
                    emb = _mean_with_mask(emb, batch[&#34;attention_mask&#34;])
                else:
                    raise Exception(f&#34;keys: {output.keys()}&#34;)

            embs.append(emb.cpu().detach().numpy())

        embs = np.concatenate(embs)

    # else:
        # raise Exception(f&#34;Unknown model checkpoint {checkpoint}&#34;)

    # sum over the embeddings
    if sum_embeddings:
        embs = embs.sum(axis=0).reshape(1, -1)
    if len(seqs) == 0:
        embs *= 0

    return {&#34;embs&#34;: embs, &#34;seq_len&#34;: len(seqs)}


def _mean_with_mask(emb_batch, mask_batch):
    &#39;&#39;&#39;Compute the mean of embeddings ignoring masked tokens
    &#39;&#39;&#39;
    # create a mask for real tokens
    expanded_attention_mask = mask_batch.unsqueeze(
        -1).expand_as(emb_batch)

    # compute the sum of embeddings for real tokens and count the real tokens
    sum_embeddings = (emb_batch * expanded_attention_mask).sum(1)
    real_token_counts = expanded_attention_mask.sum(1)

    # avoid division by zero for completely padded sequences by setting count to 1 where it&#39;s 0
    real_token_counts = real_token_counts.masked_fill_(
        real_token_counts == 0, 1)

    return sum_embeddings / real_token_counts


def _next_token_distr_with_mask(logits_batch, mask_batch):
    &#39;&#39;&#39;Return the logits of the first non-masked token
    &#39;&#39;&#39;
    # get the real token counts
    real_token_counts = mask_batch.sum(1) - 1

    # get the logits of the next token
    next_token_logits = logits_batch[
        range(len(logits_batch)), real_token_counts]

    return next_token_logits


def _get_seqs(
        example, dataset_key_text, fit_with_ngram_decomposition,
        ngrams, tokenizer_ngrams, parsing, nlp_chunks, all_ngrams, prune_stopwords) -&gt; List[str]:

    if dataset_key_text is not None:
        sentence = example[dataset_key_text]
    else:
        sentence = example

    if fit_with_ngram_decomposition:
        seqs = imodelsx.util.generate_ngrams_list(
            sentence,
            ngrams=ngrams,
            tokenizer_ngrams=tokenizer_ngrams,
            parsing=parsing,
            nlp_chunks=nlp_chunks,
            all_ngrams=all_ngrams,
            prune_stopwords=prune_stopwords,
        )
    elif isinstance(sentence, list):
        seqs = sentence
    elif isinstance(sentence, str):
        seqs = [sentence]
    else:
        raise ValueError(&#34;sentence must be a string or list of strings&#34;)

    seq_len = len(seqs)
    if seq_len == 0:
        # will multiply embedding by 0 so doesn&#39;t matter, but still want to get the shape
        seqs = [&#34;dummy&#34;]
    return seqs


def _clean_np_array(arr):
    &#34;&#34;&#34;Replace inf and nan with 0&#34;&#34;&#34;
    arr[np.isinf(arr)] = 0
    arr[np.isnan(arr)] = 0
    return arr</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodelsx.auglinear.embed.embed_and_sum_function"><code class="name flex">
<span>def <span class="ident">embed_and_sum_function</span></span>(<span>example, model, ngrams: int, tokenizer_embeddings, tokenizer_ngrams, checkpoint: str, dataset_key_text: str = None, layer: str = 'last_hidden_state', padding: str = True, batch_size: int = 8, parsing: str = '', nlp_chunks=None, all_ngrams: bool = False, fit_with_ngram_decomposition: bool = True, embedding_prefix: str = 'Represent the short phrase for sentiment classification: ', embedding_suffix: str = '', embedding_strategy: str = 'mean', sum_embeddings=True, prune_stopwords=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get summed embeddings for a single example</p>
<h2 id="params">Params</h2>
<p>ngrams: int
What order of ngrams to use (1 for unigrams, 2 for bigrams, &hellip;)
dataset_key_text:
str that identifies where data examples are stored, e.g. "sentence" for sst2
tokenizer_embeddings
tokenizing for the embedding model
tokenizer_ngrams
tokenizing the ngrams (word-based tokenization is more interpretable)
layer: str
which layer to extract embeddings from
batch_size: int
batch size for simultaneously running ngrams (for a single example)
parsing: str
whether to use parsing rather than extracting all ngrams
nlp_chunks
if parsing is not empty string, a parser that extracts specific ngrams
fit_with_ngram_decomposition
whether to fit the model with ngram decomposition (if not just use the standard sentence)
embedding_prefix
if checkpoint is an instructor/autoregressive model, prepend this prompt
embedding_suffix
if checkpoint is an autoregressive model, append this prompt
embedding_strategy: str
'mean': compute mean over ngram tokens
'next_token_distr': use next token distribution as an embedding (requires AutoModelForCausalLM checkpoint)
all_ngrams: bool
whether to include all ngrams of lower order</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed_and_sum_function(
    example,
    model,
    ngrams: int,
    tokenizer_embeddings,
    tokenizer_ngrams,
    checkpoint: str,
    dataset_key_text: str = None,
    layer: str = &#34;last_hidden_state&#34;,
    padding: str = True,
    batch_size: int = 8,
    parsing: str = &#34;&#34;,
    nlp_chunks=None,
    all_ngrams: bool = False,
    fit_with_ngram_decomposition: bool = True,
    embedding_prefix: str = &#34;Represent the short phrase for sentiment classification: &#34;,
    embedding_suffix: str = &#34;&#34;,
    embedding_strategy: str = &#39;mean&#39;,
    sum_embeddings=True,
    prune_stopwords=False,
):
    &#34;&#34;&#34;Get summed embeddings for a single example

    Params
    ------
    ngrams: int
        What order of ngrams to use (1 for unigrams, 2 for bigrams, ...)
    dataset_key_text:
        str that identifies where data examples are stored, e.g. &#34;sentence&#34; for sst2
    tokenizer_embeddings
        tokenizing for the embedding model
    tokenizer_ngrams
        tokenizing the ngrams (word-based tokenization is more interpretable)
    layer: str
        which layer to extract embeddings from
    batch_size: int
        batch size for simultaneously running ngrams (for a single example)
    parsing: str
        whether to use parsing rather than extracting all ngrams
    nlp_chunks
        if parsing is not empty string, a parser that extracts specific ngrams
    fit_with_ngram_decomposition
        whether to fit the model with ngram decomposition (if not just use the standard sentence)
    embedding_prefix
        if checkpoint is an instructor/autoregressive model, prepend this prompt
    embedding_suffix
        if checkpoint is an autoregressive model, append this prompt
    embedding_strategy: str
        &#39;mean&#39;: compute mean over ngram tokens
        &#39;next_token_distr&#39;: use next token distribution as an embedding (requires AutoModelForCausalLM checkpoint)
    all_ngrams: bool
        whether to include all ngrams of lower order
    &#34;&#34;&#34;

    # convert to list of strings
    seqs = _get_seqs(
        example, dataset_key_text, fit_with_ngram_decomposition,
        ngrams, tokenizer_ngrams, parsing, nlp_chunks, all_ngrams, prune_stopwords)
    if embedding_strategy == &#39;next_token_distr&#39;:
        seqs = [f&#39;{embedding_prefix}{x_i}{embedding_suffix}&#39; for x_i in seqs]

    if not checkpoint.startswith(&#34;hkunlp/instructor&#34;) and (
        not hasattr(tokenizer_embeddings, &#34;pad_token&#34;)
        or tokenizer_embeddings.pad_token is None
    ):
        tokenizer_embeddings.pad_token = tokenizer_embeddings.eos_token

    # compute embeddings
    embs = []
    if checkpoint.startswith(&#34;hkunlp/instructor&#34;):
        embs = model.encode(
            [[embedding_prefix, x_i] for x_i in seqs], batch_size=batch_size
        )
    else:
        tokens = tokenizer_embeddings(
            seqs, padding=padding, truncation=True, return_tensors=&#34;pt&#34;
        )

        ds = Dataset.from_dict(tokens).with_format(&#34;torch&#34;)

        for batch in DataLoader(ds, batch_size=batch_size, shuffle=False):
            batch = {k: v.to(model.device) for k, v in batch.items()}

            with torch.no_grad():
                output = model(**batch)
            torch.cuda.empty_cache()
            if embedding_strategy == &#34;next_token_distr&#34;:
                emb = _next_token_distr_with_mask(
                    output[&#34;logits&#34;], batch[&#34;attention_mask&#34;]
                )
            else:
                if layer == &#34;pooler_output&#34;:
                    emb = output[&#34;pooler_output&#34;]
                elif layer == &#34;last_hidden_state_mean&#34; or layer == &#34;last_hidden_state&#34;:
                    # extract (batch_size, seq_len, hidden_size)
                    emb = output[&#34;last_hidden_state&#34;]

                    # convert to (batch_size, hidden_size)
                    emb = _mean_with_mask(emb, batch[&#34;attention_mask&#34;])

                elif &#34;hidden_states&#34; in output.keys():
                    # extract (layer x (batch_size, seq_len, hidden_size))
                    h = output[&#34;hidden_states&#34;]

                    # convert to (batch_size, seq_len, hidden_size)
                    emb = h[0]

                    # convert to (batch_size, hidden_size)
                    emb = _mean_with_mask(emb, batch[&#34;attention_mask&#34;])
                else:
                    raise Exception(f&#34;keys: {output.keys()}&#34;)

            embs.append(emb.cpu().detach().numpy())

        embs = np.concatenate(embs)

    # else:
        # raise Exception(f&#34;Unknown model checkpoint {checkpoint}&#34;)

    # sum over the embeddings
    if sum_embeddings:
        embs = embs.sum(axis=0).reshape(1, -1)
    if len(seqs) == 0:
        embs *= 0

    return {&#34;embs&#34;: embs, &#34;seq_len&#34;: len(seqs)}</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodelsx.auglinear" href="index.html">imodelsx.auglinear</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imodelsx.auglinear.embed.embed_and_sum_function" href="#imodelsx.auglinear.embed.embed_and_sum_function">embed_and_sum_function</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>