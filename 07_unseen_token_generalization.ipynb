{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: this nb is self-contained, just requires the one saved pkl model in the results folder.\n",
    "The point is to see how the Emb-grams model does on unseen tokens at test-time.\n",
    "Specifically, we check the top-interacting words that exist in the test set but not in the train set.\n",
    "\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import datasets\n",
    "import numpy as np\n",
    "import os.path\n",
    "from spacy.lang.en import English\n",
    "from datasets import load_from_disk\n",
    "import pickle as pkl\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import analyze_helper\n",
    "import dvu\n",
    "dvu.set_style()\n",
    "import pandas as pd\n",
    "from os.path import join as oj\n",
    "import string\n",
    "from typing import List\n",
    "import data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import config\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings (model trained on whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/embedded-ngrams/.embgrams/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.24.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Using custom data configuration default\n",
      "Reusing dataset sst2 (/home/chansingh/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbd55dd4527474f9eaeabd28f300d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load embs for indivual words/bigrams\n",
    "embs = pkl.load(open(oj(config.misc_dir, \"word_embs_sst_train.pkl\"), \"rb\"))\n",
    "embs2 = pkl.load(open(oj(config.misc_dir, \"embs2_sst_top_interactions.pkl\"), \"rb\"))\n",
    "\n",
    "# also load coef data\n",
    "df = pd.read_csv(oj(config.misc_dir, \"df_unigram_sst.csv\"), index_col=0)\n",
    "df2 = pd.read_csv(oj(config.misc_dir, \"df_bigram_sst.csv\"), index_col=0)\n",
    "d = pkl.load(open(oj(config.misc_dir, \"top_interacting_words_df2.pkl\"), \"rb\"))\n",
    "d = analyze_helper.add_bert_coefs(\n",
    "    d, df, embs, embs2, cached_model=\"results/sst_bert_finetuned_ngrams=2.pkl\"\n",
    ")\n",
    "\n",
    "# get sst data\n",
    "dataset = analyze_helper.get_sst_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the unseen bigrams that yield the largest BOW coefs.\n",
    "\n",
    "These bigrams tend to be quite rare / unappealing, so we probably want to use a model that was trained on less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/embedded-ngrams/.embgrams/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get vocabs\n",
    "tok_simp = English().tokenizer\n",
    "tokenizer_func = lambda x: [str(x) for x in tok_simp(x)]\n",
    "v_train = CountVectorizer(tokenizer=tokenizer_func, ngram_range=(2, 2)).fit(\n",
    "    dataset[\"train\"][\"sentence\"]\n",
    ")\n",
    "v_test = CountVectorizer(tokenizer=tokenizer_func, ngram_range=(2, 2)).fit(\n",
    "    dataset[\"validation\"][\"sentence\"]\n",
    ")\n",
    "bigrams_train = sorted(list(v_train.vocabulary_.keys()))\n",
    "bigrams_test = sorted(list(v_test.vocabulary_.keys()))\n",
    "\n",
    "\n",
    "# calculate coefs\n",
    "matrix_test = v_test.transform(dataset['validation']['sentence'])\n",
    "tot_counts2 = pd.DataFrame(matrix_test.sum(axis=0), columns=v_test.get_feature_names())\n",
    "m_test = LogisticRegressionCV()\n",
    "m_test.fit(matrix_test, dataset['validation']['label'])\n",
    "coef_test = m_test.coef_.flatten() # note -- coef has not been mapped to same idxs as words\n",
    "\n",
    "# sort to put largest coefs at top\n",
    "idxs_sorted = np.argsort(np.abs(coef_test))[::-1]\n",
    "bigrams_test_sorted = np.array(bigrams_test)[idxs_sorted]\n",
    "coef_test_sorted = coef_test[idxs_sorted]\n",
    "\n",
    "# filter by unseen\n",
    "idxs_unseen = list(map(lambda x: x not in bigrams_train, bigrams_test_sorted))\n",
    "bigrams_unseen_sorted = bigrams_test_sorted[idxs_unseen]\n",
    "coef_unseen_sorted = coef_test_sorted[idxs_unseen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flair . 0.03577738959642746\n",
      "charm to -0.0505954592645493\n",
      "deep and -0.08153103409847423\n",
      "too , 0.09298732605922387\n",
      "who love -0.03619972128826575\n",
      "a giggle 0.028262171263907686\n",
      "a minute 0.05489001068525908\n",
      "- this -0.06006566340714442\n",
      "mesmerizing . 0.08137781419676555\n",
      "cool ? 0.05489001068525908\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(bigrams_unseen_sorted[i], coef_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgrams': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1f8dbe53c838cb92e1131e4b0e751bd3399f29814a546969a59bd338376d193"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
