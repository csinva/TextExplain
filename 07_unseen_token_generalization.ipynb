{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: this nb is self-contained, just requires the one saved pkl model in the results folder.\n",
    "The point is to see how the Emb-grams model does on unseen tokens at test-time.\n",
    "Specifically, we check the top-interacting words that exist in the test set but not in the train set.\n",
    "\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import datasets\n",
    "import numpy as np\n",
    "import os.path\n",
    "from spacy.lang.en import English\n",
    "from datasets import load_from_disk\n",
    "import pickle as pkl\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import analyze_helper\n",
    "import dvu\n",
    "dvu.set_style()\n",
    "import pandas as pd\n",
    "from os.path import join as oj\n",
    "import string\n",
    "from typing import List\n",
    "import data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import config\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings (and more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/embedded-ngrams/.embgrams/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.24.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load embs for indivual words/bigrams\n",
    "embs = pkl.load(open(oj(config.misc_dir, 'word_embs_sst_train.pkl'), 'rb'))\n",
    "embs2 = pkl.load(open(oj(config.misc_dir, 'embs2_sst_top_interactions.pkl'), 'rb'))\n",
    "\n",
    "# also load coef data\n",
    "df = pd.read_csv(oj(config.misc_dir, 'df_unigram_sst.csv'), index_col=0)\n",
    "df2 = pd.read_csv(oj(config.misc_dir, 'df_bigram_sst.csv'), index_col=0)\n",
    "d = pkl.load(open(oj(config.misc_dir, 'top_interacting_words_df2.pkl'), 'rb'))\n",
    "d = analyze_helper.add_bert_coefs(d, df, embs, embs2)\n",
    "\n",
    "# get sst data\n",
    "dataset = analyze_helper.get_sst_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>interaction_score</th>\n",
       "      <th>tot_counts</th>\n",
       "      <th>coef</th>\n",
       "      <th>coef1</th>\n",
       "      <th>coef2</th>\n",
       "      <th>unigram1</th>\n",
       "      <th>unigram2</th>\n",
       "      <th>bert_coef_bigram</th>\n",
       "      <th>bert_coef_unigram1</th>\n",
       "      <th>bert_coef_unigram2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44950</th>\n",
       "      <td>not bad</td>\n",
       "      <td>13.26</td>\n",
       "      <td>7</td>\n",
       "      <td>6.46</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-3.61</td>\n",
       "      <td>not</td>\n",
       "      <td>bad</td>\n",
       "      <td>0.350199</td>\n",
       "      <td>-0.337425</td>\n",
       "      <td>-0.796018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57713</th>\n",
       "      <td>spielberg calls</td>\n",
       "      <td>13.04</td>\n",
       "      <td>7</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.81</td>\n",
       "      <td>9.32</td>\n",
       "      <td>spielberg</td>\n",
       "      <td>calls</td>\n",
       "      <td>-0.214195</td>\n",
       "      <td>0.375404</td>\n",
       "      <td>-0.783104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43880</th>\n",
       "      <td>n't lost</td>\n",
       "      <td>13.02</td>\n",
       "      <td>14</td>\n",
       "      <td>4.86</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-4.51</td>\n",
       "      <td>n't</td>\n",
       "      <td>lost</td>\n",
       "      <td>0.511850</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>-0.889112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44408</th>\n",
       "      <td>never lacks</td>\n",
       "      <td>12.68</td>\n",
       "      <td>4</td>\n",
       "      <td>4.27</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-7.31</td>\n",
       "      <td>never</td>\n",
       "      <td>lacks</td>\n",
       "      <td>0.793433</td>\n",
       "      <td>-0.312836</td>\n",
       "      <td>-1.294468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>banal bore</td>\n",
       "      <td>12.44</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>banal</td>\n",
       "      <td>bore</td>\n",
       "      <td>-1.196036</td>\n",
       "      <td>-0.872826</td>\n",
       "      <td>-1.263071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bigram  interaction_score  tot_counts  coef  coef1  coef2  \\\n",
       "44950          not bad              13.26           7  6.46  -3.19  -3.61   \n",
       "57713  spielberg calls              13.04           7  0.09   3.81   9.32   \n",
       "43880         n't lost              13.02          14  4.86  -3.65  -4.51   \n",
       "44408      never lacks              12.68           4  4.27  -1.10  -7.31   \n",
       "14407       banal bore              12.44           4 -2.45  -4.88 -10.02   \n",
       "\n",
       "        unigram1 unigram2  bert_coef_bigram  bert_coef_unigram1  \\\n",
       "44950        not      bad          0.350199           -0.337425   \n",
       "57713  spielberg    calls         -0.214195            0.375404   \n",
       "43880        n't     lost          0.511850           -0.394974   \n",
       "44408      never    lacks          0.793433           -0.312836   \n",
       "14407      banal     bore         -1.196036           -0.872826   \n",
       "\n",
       "       bert_coef_unigram2  \n",
       "44950           -0.796018  \n",
       "57713           -0.783104  \n",
       "43880           -0.889112  \n",
       "44408           -1.294468  \n",
       "14407           -1.263071  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the unseen bigrams that yield the largest BOW coefs.**\n",
    "\n",
    "These bigrams tend to be quite rare / unappealing, so we probably want to use a model that was trained on less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/embedded-ngrams/.embgrams/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get vocabs\n",
    "tok_simp = English().tokenizer\n",
    "tokenizer_func = lambda x: [str(x) for x in tok_simp(x)]\n",
    "v_train = CountVectorizer(tokenizer=tokenizer_func, ngram_range=(2, 2)).fit(\n",
    "    dataset[\"train\"][\"sentence\"]\n",
    ")\n",
    "v_test = CountVectorizer(tokenizer=tokenizer_func, ngram_range=(2, 2)).fit(\n",
    "    dataset[\"validation\"][\"sentence\"]\n",
    ")\n",
    "bigrams_train = sorted(list(v_train.vocabulary_.keys()))\n",
    "bigrams_test = sorted(list(v_test.vocabulary_.keys()))\n",
    "\n",
    "\n",
    "# calculate coefs\n",
    "matrix_test = v_test.transform(dataset['validation']['sentence'])\n",
    "tot_counts2 = pd.DataFrame(matrix_test.sum(axis=0), columns=v_test.get_feature_names())\n",
    "m_test = LogisticRegressionCV()\n",
    "m_test.fit(matrix_test, dataset['validation']['label'])\n",
    "coef_test = m_test.coef_.flatten() # note -- coef has not been mapped to same idxs as words\n",
    "\n",
    "# sort to put largest coefs at top\n",
    "idxs_sorted = np.argsort(np.abs(coef_test))[::-1]\n",
    "bigrams_test_sorted = np.array(bigrams_test)[idxs_sorted]\n",
    "coef_test_sorted = coef_test[idxs_sorted]\n",
    "\n",
    "# filter by unseen\n",
    "idxs_unseen = list(map(lambda x: x not in bigrams_train, bigrams_test_sorted))\n",
    "bigrams_unseen_sorted = bigrams_test_sorted[idxs_unseen]\n",
    "coef_unseen_sorted = coef_test_sorted[idxs_unseen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flair . 0.03577738959642746\n",
      "charm to -0.0505954592645493\n",
      "deep and -0.08153103409847423\n",
      "too , 0.09298732605922387\n",
      "who love -0.03619972128826575\n",
      "a giggle 0.028262171263907686\n",
      "a minute 0.05489001068525908\n",
      "- this -0.06006566340714442\n",
      "mesmerizing . 0.08137781419676555\n",
      "cool ? 0.05489001068525908\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(bigrams_unseen_sorted[i], coef_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgrams': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1f8dbe53c838cb92e1131e4b0e751bd3399f29814a546969a59bd338376d193"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
