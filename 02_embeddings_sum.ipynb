{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from spacy.lang.en import English\n",
    "import argparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import sklearn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to aggregate embeddings run for a particular order of n-gram here (we'll just add them up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '/scratch/users/vision/chandan/embedded-ngrams/data/processed'\n",
    "dir_names = [f for f in sorted(os.listdir(processed_dir))\n",
    "             if not '-all' in f\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 5762.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying for ngram=10_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3, 9]\n",
      "Trying for ngram=10_sub=1000_bert-base-uncased\n",
      "\tmissing small_ngrams:  [9]\n",
      "Trying for ngram=10_sub=100_bert-base-uncased\n",
      "\tmissing small_ngrams:  [9]\n",
      "Trying for ngram=10_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [9]\n",
      "Trying for ngram=2_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=2_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=2_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=2_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=2_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=3_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=3_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=3_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=3_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=4_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3]\n",
      "Trying for ngram=4_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=4_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=4_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=4_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=5_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3]\n",
      "Trying for ngram=5_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=5_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=5_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=5_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=6_sub=-1_bert-base-uncased\n",
      "\tmissing small_ngrams:  [2, 3, 4, 5]\n",
      "Trying for ngram=6_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3]\n",
      "Trying for ngram=6_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=6_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=6_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=6_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=7_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3]\n",
      "Trying for ngram=7_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=7_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=7_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=7_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=8_sub=-1_bert-base-uncased\n",
      "\tmissing small_ngrams:  [2, 3, 4, 5, 7]\n",
      "Trying for ngram=8_sub=-1_textattack-bert-base-uncased-SST-2\n",
      "\tmissing small_ngrams:  [3]\n",
      "Trying for ngram=8_sub=1000_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=8_sub=1000_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n",
      "Trying for ngram=8_sub=100_bert-base-uncased\n",
      "\tdone already!\n",
      "Trying for ngram=8_sub=100_textattack-bert-base-uncased-SST-2\n",
      "\tdone already!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dir_names))):\n",
    "    s = dir_names[i]\n",
    "    start = s.index('=') + 1\n",
    "    end = s.index('_')\n",
    "    num = int(s[ start: end])\n",
    "    if num > 1:\n",
    "        print('Trying for', s)\n",
    "        s_new = s + '-all'\n",
    "        if os.path.exists(oj(processed_dir, s_new)):\n",
    "            print('\\tdone already!')\n",
    "            continue\n",
    "            \n",
    "        pre = s[:start]\n",
    "        end = s[end:]\n",
    "        all_exist = True\n",
    "        num_missing = []\n",
    "        for num_small in range(1, num):\n",
    "            fname_small = pre + str(num_small) + end\n",
    "            all_exist = all_exist and (fname_small in dir_names)\n",
    "            if not fname_small in dir_names:\n",
    "                num_missing.append(num_small)\n",
    "        \n",
    "        if not all_exist:\n",
    "            print('\\tmissing small_ngrams: ', num_missing)\n",
    "            continue\n",
    "            \n",
    "        # load dset\n",
    "        reloaded_dataset = load_from_disk(oj(processed_dir, s))\n",
    "        X_train = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "        X_val = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "            \n",
    "        for num_small in range(1, num):\n",
    "            fname_small = pre + str(num_small) + end            \n",
    "            reloaded_dataset = load_from_disk(oj(processed_dir, fname_small))\n",
    "            X_train_small = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "            X_val_small = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "\n",
    "            X_train += X_train_small\n",
    "            X_val += X_val_small\n",
    "\n",
    "        os.makedirs(oj(processed_dir, s_new), exist_ok=True)\n",
    "        mu = X_train.mean(axis=0)\n",
    "        sigma = X_train.std(axis=0)\n",
    "        r = {\n",
    "            'X_train': X_train,\n",
    "            'X_val': X_val,\n",
    "            'mean': mu,\n",
    "            'sigma': sigma,\n",
    "        }\n",
    "        pkl.dump(r, open(oj(processed_dir, s_new, 'data.pkl'), 'wb'))\n",
    "        print('\\tsuccess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
