{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from spacy.lang.en import English\n",
    "import argparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import sklearn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to aggregate embeddings run for a particular order of n-gram here (we'll just add them up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = 'data/processed'\n",
    "dir_names = [f for f in sorted(os.listdir(processed_dir))\n",
    "             if not '-all' in f\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dir_names))):\n",
    "    s = dir_names[i]\n",
    "    start = s.index('=') + 1\n",
    "    end = s.index('_')\n",
    "    num = int(s[ start: end])\n",
    "    if num > 1:\n",
    "        print('Trying for', s)\n",
    "        s_new = s + '-all'\n",
    "        if os.path.exists(oj(processed_dir, s_new)):\n",
    "            print('\\tdone already!')\n",
    "            continue\n",
    "        pre = s[:start]\n",
    "        end = s[end:]\n",
    "        reloaded_dataset = load_from_disk(oj(processed_dir, s))\n",
    "        X_train = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "        X_val = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "        num_smalls = []\n",
    "        for num_small in range(1, num):\n",
    "            fname_small = pre + str(num_small) + end\n",
    "            if fname_small in dir_names:\n",
    "                reloaded_dataset = load_from_disk(oj(processed_dir, fname_small))\n",
    "                X_train_small = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "                X_val_small = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "                num_smalls.append(num_small)\n",
    "                \n",
    "                X_train += X_train_small\n",
    "                X_val += X_val_small\n",
    "        if num_smalls == list(range(1, num)):\n",
    "            os.makedirs(oj(processed_dir, s_new), exist_ok=True)\n",
    "            mu = X_train.mean(axis=0)\n",
    "            sigma = X_train.std(axis=0)\n",
    "            r = {\n",
    "                'X_train': X_train,\n",
    "                'X_val': X_val,\n",
    "                'mean': mu,\n",
    "                'sigma': sigma,\n",
    "            }\n",
    "            pkl.dump(r, open(oj(processed_dir, s_new, 'data.pkl'), 'wb'))\n",
    "            print('\\tsuccess!')\n",
    "        else:\n",
    "            print('\\tfailed!', num_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39971717,  0.47728783,  0.46848973, ...,  0.32394024,\n",
       "         0.40770636, -0.41445175],\n",
       "       [ 0.37845816,  0.27007233, -0.6737011 , ..., -0.7822047 ,\n",
       "         0.34070138, -0.36811865],\n",
       "       [-0.28291333, -0.26231812, -1.51527908, ..., -1.70423174,\n",
       "        -0.27811537,  0.23633958],\n",
       "       ...,\n",
       "       [ 0.59832025,  0.59225591,  0.1777626 , ...,  0.29190491,\n",
       "         0.60562303, -0.59645348],\n",
       "       [ 0.72135488,  0.72851596,  0.32479591, ...,  0.17907882,\n",
       "         0.70840679, -0.71539313],\n",
       "       [-0.94252311, -0.62276438, -0.20464616, ...,  0.7566754 ,\n",
       "        -0.82595086,  0.91553885]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train - mean) / X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
