{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from spacy.lang.en import English\n",
    "import argparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import sklearn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to aggregate embeddings run for a particular order of n-gram here (we'll just add them up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = 'data/processed'\n",
    "dir_names = [f for f in sorted(os.listdir(processed_dir))\n",
    "             if not '-all' in f\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying for ngram=10_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:47,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfailed! [1, 2, 3, 4, 5, 7]\n",
      "Trying for ngram=10_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:05<00:31,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfailed! [1, 2, 3, 4, 5, 7]\n",
      "Trying for ngram=2_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:06<00:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=2_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:06<00:05,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=3_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:08<00:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=3_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:09<00:05,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=4_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:11<00:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=4_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:12<00:04,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=5_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:14<00:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=5_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:16<00:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsuccess!\n",
      "Trying for ngram=7_sub=1000_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:19<00:01,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfailed! [1, 2, 3, 4, 5]\n",
      "Trying for ngram=7_sub=100_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:20<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfailed! [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dir_names))):\n",
    "    s = dir_names[i]\n",
    "    start = s.index('=') + 1\n",
    "    end = s.index('_')\n",
    "    num = int(s[ start: end])\n",
    "    if num > 1:\n",
    "        print('Trying for', s)\n",
    "        pre = s[:start]\n",
    "        end = s[end:]\n",
    "        reloaded_dataset = load_from_disk(oj(processed_dir, s))\n",
    "        X_train = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "        X_val = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "        num_smalls = []\n",
    "        for num_small in range(1, num):\n",
    "            fname_small = pre + str(num_small) + end\n",
    "            if fname_small in dir_names:\n",
    "                reloaded_dataset = load_from_disk(oj(processed_dir, fname_small))\n",
    "                X_train_small = np.array(reloaded_dataset['train']['embs']).squeeze()\n",
    "                X_val_small = np.array(reloaded_dataset['validation']['embs']).squeeze()\n",
    "                num_smalls.append(num_small)\n",
    "                \n",
    "                X_train += X_train_small\n",
    "                X_val += X_val_small\n",
    "        if num_smalls == list(range(1, num)):\n",
    "            s_new = s + '-all'\n",
    "            os.makedirs(oj(processed_dir, s_new), exist_ok=True)\n",
    "            mu = X_train.mean(axis=0)\n",
    "            sigma = X_train.std(axis=0)\n",
    "            r = {\n",
    "                'X_train': X_train,\n",
    "                'X_val': X_val,\n",
    "                'mean': mu,\n",
    "                'sigma': sigma,\n",
    "            }\n",
    "            pkl.dump(r, open(oj(processed_dir, s_new, 'data.pkl'), 'wb'))\n",
    "            print('\\tsuccess!')\n",
    "        else:\n",
    "            print('\\tfailed!', num_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39971717,  0.47728783,  0.46848973, ...,  0.32394024,\n",
       "         0.40770636, -0.41445175],\n",
       "       [ 0.37845816,  0.27007233, -0.6737011 , ..., -0.7822047 ,\n",
       "         0.34070138, -0.36811865],\n",
       "       [-0.28291333, -0.26231812, -1.51527908, ..., -1.70423174,\n",
       "        -0.27811537,  0.23633958],\n",
       "       ...,\n",
       "       [ 0.59832025,  0.59225591,  0.1777626 , ...,  0.29190491,\n",
       "         0.60562303, -0.59645348],\n",
       "       [ 0.72135488,  0.72851596,  0.32479591, ...,  0.17907882,\n",
       "         0.70840679, -0.71539313],\n",
       "       [-0.94252311, -0.62276438, -0.20464616, ...,  0.7566754 ,\n",
       "        -0.82595086,  0.91553885]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train - mean) / X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
